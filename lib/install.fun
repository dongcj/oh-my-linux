#!/usr/bin/env bash
#
# Author: dongcj <ntwk@163.com>
# description: 
#

######################################################################
# 作用: 生成固定的uuid(如果环境无变化)
# 用法: Ceph_UUID_Gen
# 注意：ceph name:port 生成ceph uuid
######################################################################
Ceph_UUID_Gen() {
    
    # make a UUID using an MD5 hash of a namespace UUID and a name
    baseuuid=`python -c "import uuid; print uuid.uuid3(uuid.NAMESPACE_DNS, 'shencloudtech.com')"`
    
    # get uniq node uuid
    nodeuuid=`python -c "import uuid; print uuid.getnode()"`
    
    # uniq uuid
    uniquuid=`python -c "import uuid; print uuid.uuid3(uuid.UUID(\"$baseuuid\"), \"$nodeuuid\")"`
    
    # sha1 nodeuuid & $FULL_DOMAIN:$CEPH_CLUSTERNAME:$CEPH_MON_PORT
    uuid_cluster=`python -c "import uuid; print uuid.uuid5(uuid.UUID(\"$uniquuid\"), \"$FULL_DOMAIN:$CEPH_CLUSTERNAME:$CEPH_MON_PORT\")"`
    
    echo $uuid_cluster
    
}



######################################################################
# 作用: 根据OSD数量, 计算PG和PGP
# 用法: Ceph_PG_Calc <OSD_NUMS>
# 注意：
#           (OSDs * 100)
#Total PGs = ------------
#              Replicas
######################################################################
Ceph_PG_Calc() {

    if ! Checker is_allnum "$1"; then
        Log ERROR 'Function Ceph_PG_Calc() error! Usage: Ceph_PG_Calc $OSD_NUM\n'
    fi
    osd_num=$1
    
    pg_num=`echo "scale=0; $osd_num * 100 / $CEPH_OSD_POOL_DEFAULT_SIZE" | bc`
    
    echo "$pg_num"

}


######################################################################
# 作用: 渲染ceph.conf, 并将ceph.conf分发至所有服务器上
# 用法: Ceph_Conf_Generater
# 注意：分发给不同服务器时，需要根据服务器不同网络修改
######################################################################
Ceph_Conf_Generater() {

    
    # generate id
    CLUSTER_ID=`Ceph_UUID_Gen`
    
    # from setting.conf
    CLUSTER_NAME=$CEPH_CLUSTERNAME
    
    # Some variable has been already generated by function "ANS_Advanced_Value_Get"


    # osd pool size
    CEPH_OSD_POOL_DEFAULT_SIZE=${CEPH_OSD_POOL_DEFAULT_SIZE:-3}
    CEPH_OSD_POOL_DEFAULT_MIN_SIZE=${CEPH_OSD_POOL_DEFAULT_MIN_SIZE:-2}
    

    
    # 为Conf_Replacer构造以下循环所需的变量
    for i in $MON_HOST; do 
        ip=`grep $i <<<"$HOST_CONTENT" | awk -F'=' '{print $2}' | tr -d '"' | awk -F'|' '{print $2}' | awk -F',' '{print $1}'`
        MON_HOST_IP_PORT="$MON_HOST_IP_PORT $i,$ip,$CEPH_MON_PORT"
    done
    
    #Log DEBUG "MON_HOST_IP_PORT=$MON_HOST_IP_PORT"
    Log DEBUG "render the ceph template file to ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON}"

    # generate the ceph.conf on the deploy host (DO NOT replace PUBLIC_NETWORK now)
    Conf_Replacer ${CONF_DIR}/$CEPH_CONF_TEMPLATE ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON} CLUSTER_ID CLUSTER_NAME MON_HOST_LIST \
    MON_IPADDR_PORT  CLUSTER_NETWORK  CEPH_OSD_POOL_DEFAULT_SIZE  CEPH_OSD_POOL_DEFAULT_MIN_SIZE
    
    
    # PUBLIC_NETWORK
    Log DEBUG "distribute ceph MON conf file to all hosts"
    for i in $HOST_CONTENT; do
        this_hostname=`echo $i | awk -F'=' '{print $1}'`
        this_ipaddr=`echo $i | awk -F'=' '{print $2}' | tr -d '"' | awk -F'|' '{print $2}' | awk -F',' '{print $1}'`
        this_mask=`echo $i | awk -F'=' '{print $2}' | tr -d '"' | awk -F'|' '{print $2}' | awk -F',' '{print $2}'`
        this_network=`IP_Mask_To_Network $this_ipaddr $this_mask`
        this_mask_cdr=`Mask_To_CDR $this_mask`
        
        PUBLIC_NETWORK=${this_network}/$this_mask_cdr
        
        # update PUBLIC_NETWORK
        Conf_Replacer ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON} ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON}.$this_hostname PUBLIC_NETWORK
    
        # rsync file
        Rsync_File "$this_hostname" "${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON}.$this_hostname" "${CEPH_CONF}"
        
        # rm file
        rm -rf ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON}.$this_hostname
        
    done

}



######################################################################
# 作用: 根据生成的 CEPH_OSD_ASSIGN_FILE 中参数渲染${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON}
# 用法: Ceph_Conf_Update_OSD <CEPH_OSD_ASSIGN_FILE>
# 注意：需要用MON生成的文件做为原始文件，再进行进一步替换
######################################################################
Ceph_Conf_Update_OSD() {

    infofile=${RUN_DIR}/$CEPH_OSD_ASSIGN_FILE
    
    if [ ! -f $infofile ]; then
        Log ERROR "Function Usage: Ceph_Conf_Update_OSD <CEPH_OSD_ASSIGN_FILE>\n"
    fi
    
    # get the all osd host
    all_osd_host=`INI_Parser $infofile`
    
    # 为Conf_Replacer构造以下循环所需的变量
    for i in $all_osd_host; do 
    
        # skip "add-more-osd"
        if [ "$i" = "add-more-osd" ]; then 
            continue
        fi
        osd_number=`INI_Parser $infofile $i osd_number`
        osd_number_begin=`echo $osd_number | awk -F'-' '{print $1}'`
        osd_number_end=`echo $osd_number | awk -F'-' '{print $2}'`
        
        for ((j=$osd_number_begin; j<=$osd_number_end; j++)); do
            OSDNUM_HOST_LIST="$OSDNUM_HOST_LIST $j,$i"
        done
    done
    
    OSD_NUMS=`echo $OSDNUM_HOST_LIST | wc -w`
    
    # calc pg/pgp num
    OSD_POOL_PG_NUM=`Ceph_PG_Calc $OSD_NUMS`
    OSD_POOL_PGP_NUM=$OSD_POOL_PG_NUM
    
    #Log DEBUG "OSDNUM_HOST_LIST=$OSDNUM_HOST_LIST"
    Log DEBUG "render the ceph template file to ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_OSD}"

    # generate the ceph.conf on the deploy host
    Conf_Replacer ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON} ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_OSD} \
    OSD_POOL_PG_NUM OSD_POOL_PGP_NUM 


    # PUBLIC_NETWORK
    Log DEBUG "distribute ceph OSD conf file to all hosts"
    for i in $HOST_CONTENT; do
        this_hostname=`echo $i | awk -F'=' '{print $1}'`
        this_ipaddr=`echo $i | awk -F'=' '{print $2}' | tr -d '"' | awk -F'|' '{print $2}' | awk -F',' '{print $1}'`
        this_mask=`echo $i | awk -F'=' '{print $2}' | tr -d '"' | awk -F'|' '{print $2}' | awk -F',' '{print $2}'`
        this_network=`IP_Mask_To_Network $this_ipaddr $this_mask`
        this_mask_cdr=`Mask_To_CDR $this_mask`
        
        PUBLIC_NETWORK=${this_network}/$this_mask_cdr
        
        # update PUBLIC_NETWORK
        Conf_Replacer ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_OSD} ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_OSD}.$this_hostname PUBLIC_NETWORK
    
        # rsync file
        Rsync_File "$this_hostname" "${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_OSD}.$this_hostname" "${CEPH_CONF}"
        
        # rm file
        rm -rf ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_OSD}.$this_hostname
    done

}


######################################################################
# 作用: 根据 MDS HOST 渲染${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_OSD}
# 用法: Ceph_Conf_Update_MDS
# 注意：
######################################################################
Ceph_Conf_Update_MDS() {
    
    starter=0
    # MDSNUM_HOST_LIST
    for i in ${MDS_HOST}; do
    
        # make id,hostname list
        MDSNUM_HOST_LIST="$MDSNUM_HOST_LIST ${starter},{$i}"
        
        starter=$((starter + 1))
    done
    
    #Log DEBUG "OSDNUM_HOST_LIST=$OSDNUM_HOST_LIST"
    Log DEBUG "render the ceph template file to ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MDS}"

    # generate the ceph.conf on the deploy host
    Conf_Replacer ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_OSD} ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MDS} 

    # PUBLIC_NETWORK
    Log DEBUG "distribute ceph MDS conf file to all hosts"
    for i in $HOST_CONTENT; do
        this_hostname=`echo $i | awk -F'=' '{print $1}'`
        this_ipaddr=`echo $i | awk -F'=' '{print $2}' | tr -d '"' | awk -F'|' '{print $2}' | awk -F',' '{print $1}'`
        this_mask=`echo $i | awk -F'=' '{print $2}' | tr -d '"' | awk -F'|' '{print $2}' | awk -F',' '{print $2}'`
        this_network=`IP_Mask_To_Network $this_ipaddr $this_mask`
        this_mask_cdr=`Mask_To_CDR $this_mask`
        
        PUBLIC_NETWORK=${this_network}/$this_mask_cdr
        
        # update PUBLIC_NETWORK
        Conf_Replacer ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MDS} ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MDS}.$this_hostname PUBLIC_NETWORK
    
        # rsync file
        Rsync_File "$this_hostname" "${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MDS}.$this_hostname" "${CEPH_CONF}"
        
        # rm file
        rm -rf ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MDS}.$this_hostname
    done

}


######################################################################
# 作用: create ceph user if ceph user is not ceph
# 用法: Ceph_User_Create <RUNFILE>
# 注意：
######################################################################
Ceph_User_Create() {
    # ceph user is already installed by ceph rpm packages
    if ! id $CEPH_USER >/dev/null 2>&1; then
        Log DEBUG "ceph user $CEPH_USER already existe.."
        # check the CEPH_UID
        if [ $CEPH_UID != `id -u $CEPH_USER` -o $CEPH_GID != `id -g $CEPH_USER` ] ; then
            Log ERROR "ceph uid or gid error"
        fi
    else
        # create group & user
        Log DEBUG "create user $CEPH_USER with uid=$CEPH_UID gid=$CEPH_GID"
        Run groupadd -g $CEPH_GID $CEPH_GROUP >/dev/null 2>&1
        Run useradd -d /var/lib/ceph -g $CEPH_GROUP -m -k /etc/skel -u $CEPH_UID -s /sbin/nologin $CEPH_USER >/dev/null 2>&1
        Check_Return  $0 $LINENO $FUNCNAME
        Log SUCC "create user $CEPH_USER successful"
    fi
}



######################################################################
# 作用: 返回本服务器上运行的Cluster的所有状态, 并写入tmp目录中,push回ceph-ai
# 用法: Ceph_Cluster_Status
# 注意：
######################################################################
Ceph_Cluster_Status() {

    installed_cluster=`find /var/lib/ceph/mon/ -maxdepth 1 -type d | sed -n "s#/var/lib/ceph/mon/\(.*\)-$(hostname -s)#\1#p" | xargs`
    >${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}
    if [ -z "$installed_cluster" ]; then
        Log DEBUG "There is no installed cluster"
        # wirte to run dir
        cat <<EOF >>${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}
    [${MY_HOSTNAME}]
        clustername=
        port=
        pid=
        status=noinstall
EOF
        Log DEBUG "write cluster status to ${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}"
        return 0
    fi
    
    for i in $installed_cluster; do
        pid=`ps -ef | grep -w "ceph-mon -f --cluster $i" | grep -v grep | awk '{print $2}'`
        if [ -n "$pid" ] ; then
            # get the port from netstat
            port=`netstat -tupnl | grep $pid | grep -v grep | awk '{print $4}' | awk -F':' '{print $2}'`
            Log DEBUG "clustername=$i; port=$port; pid=$pid; status=running"

            # wirte to run dir
            cat <<EOF >>${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}
        [${MY_HOSTNAME}]
            clustername=$i
            port=$port
            pid=$pid
            status=running
EOF
            Log DEBUG "write cluster status to ${TMP_DIR}/$CEPH_CLUSTER_STATUS_FILE"
            
        else
            Log DEBUG "clustername=$i; port=null; pid=null; status=stopped"
            
            # get the port form ceph conf file
            this_conf=/etc/ceph/${i}.conf
            if [ -f $this_conf ]; then
                mon_addr_port=`INI_Parser $this_conf mon "mon addr"`
                port=`echo $mon_addr_port | awk -F',' '{print $1}' | awk -F':' '{print $2}'`   
                Log DEBUG "found the ceph cluster $i port=$port in $this_conf"
            else
                Log DEBUG "no $this_conf found, can not get port of cluster $i"
            
            fi
            # wirte to run dir
            cat <<EOF >>${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}
        [${MY_HOSTNAME}]    
            clustername=$i
            port=$port
            pid=
            status=stoped
            
EOF
            Log DEBUG "write cluster status to ${TMP_DIR}/$CEPH_CLUSTER_STATUS_FILE"
            
        fi
        
    done

}



######################################################################
# 作用: re-generate systemd for mon service, use base ceph-mon@service
# 用法: Ceph_MON_Create_Service
# 注意：
######################################################################
Ceph_MON_Create_Service() {
    
    Log DEBUG "creating MON service file"
    system_service_dir="/usr/lib/systemd/system"
    
    # src is default ceph
    src_mon_target="ceph-mon.target"
    src_mon_service_instance="ceph-mon@.service"

    # dst is ${CEPH_CLUSTERNAME}
    dst_mon_target="${CEPH_CLUSTERNAME}-mon.target"
    dst_mon_service_instance="${CEPH_CLUSTERNAME}-mon@.service"
    
    # copy from src to dst(if CEPH_CLUSTERNAME is not ceph)
    if [ "$src_mon_target" != "$dst_mon_target" ]; then
        cp -rLfap ${system_service_dir}/${src_mon_target} ${system_service_dir}/${dst_mon_target}
    fi
    if [ "$src_mon_service_instance" != "$dst_mon_service_instance" ]; then
        cp -rLfap ${system_service_dir}/${src_mon_service_instance} ${system_service_dir}/${dst_mon_service_instance}
    fi
    
    src_partof="ceph-mon.target"
    dst_partof="${CEPH_CLUSTERNAME}-mon.target"
    src_wantby="ceph-mon.target"
    dst_wantby="${CEPH_CLUSTERNAME}-mon.target"

    # replace the "Environment=CLUSTER=ceph"
    sed -i "s/Environment=CLUSTER=ceph/Environment=CLUSTER=${CEPH_CLUSTERNAME}/" \
    ${system_service_dir}/${dst_mon_service_instance}
    
    # replace the "--setuser ceph --setgroup ceph"
    sed -i "s/--setuser \(.*\) --setgroup \(.*\)/--setuser $CEPH_USER --setgroup ${CEPH_GROUP}/" \
    ${system_service_dir}/${dst_mon_service_instance}
    
    # replate the PartOf & WantedBy
    sed -i "s/PartOf=$src_partof/PartOf=$dst_partof/" ${system_service_dir}/${dst_mon_service_instance}
    sed -i "s/WantedBy=$src_wantby/WantedBy=$dst_wantby/" ${system_service_dir}/${dst_mon_service_instance}

    # auto start the service
    systemctl enable ceph.target
    systemctl enable ${CEPH_CLUSTERNAME}-mon.target
    systemctl enable ${CEPH_CLUSTERNAME}-mon@${MY_HOSTNAME}
    
    
    Check_Return  $0 $LINENO $FUNCNAME
    Log SUCC "create MON service file successful"

}





######################################################################
# 作用: remove systemd service for mon service
# 用法: Ceph_MON_Remove_Service
# 注意：
######################################################################
Ceph_MON_Remove_Service() {

    # disable the service
    Log DEBUG "disable MON service for cluster $CEPH_CLUSTERNAME"
    for i in `systemctl --plain | grep "${CEPH_CLUSTERNAME}-mon@" | awk '{print $1}'`; do 
        Run systemctl disable $i
    done
    
    # flush the service instance
    systemctl reset-failed
    
    systemctl disable ${CEPH_CLUSTERNAME}-mon.target
    
    Log DEBUG "remove MON service file for cluster $CEPH_CLUSTERNAME"
    system_service_dir="/usr/lib/systemd/system"

    # dst is ${CEPH_CLUSTERNAME}
    dst_mon_target="${CEPH_CLUSTERNAME}-mon.target"
    dst_mon_service_instance="${CEPH_CLUSTERNAME}-mon@.service"

    # if clustername is ceph, do not delete the service file
    if [ "$CEPH_CLUSTERNAME" != "ceph" ]; then
        if [ -f "${system_service_dir}/${dst_mon_target}" ]; then
            rm -rf ${system_service_dir}/${dst_mon_target}
            Log DEBUG "remove ${system_service_dir}/${dst_mon_target}"
        else
            Log DEBUG "${system_service_dir}/${dst_mon_target} does not exist"
        fi
        
        if [ -f "${system_service_dir}/${dst_mon_service_instance}" ]; then
            rm -rf ${system_service_dir}/${dst_mon_service_instance}
            Log DEBUG "remove ${system_service_dir}/${dst_mon_service_instance}"
        else
            Log DEBUG "${system_service_dir}/${dst_mon_service_instance} does not exist"
        fi
    else
        Log DEBUG "clustername is $CEPH_CLUSTERNAME, no need to delete"
    fi
    
    Log SUCC "remove MON service success"
    
    
}




######################################################################
# 作用: start systemd mon service
# 用法: Ceph_MON_Start_Service
# 注意：
######################################################################
Ceph_MON_Start_Service() {
    Run systemctl start ${CEPH_CLUSTERNAME}-mon@${MY_HOSTNAME}
    sleep 5
}




######################################################################
# 作用: stop systemd mon service
# 用法: Ceph_MON_Stop_Service
# 注意：
######################################################################
Ceph_MON_Stop_Service() {
    Log DEBUG "stopping MON service"
    if systemctl stop ${CEPH_CLUSTERNAME}-mon@${MY_HOSTNAME}; then
        sleep 3
        Log SUCC "stop MON service successful"
    else
        Log WARN "systemctl stop ${CEPH_CLUSTERNAME}-mon@${MY_HOSTNAME} failed, force kill"
        pid_to_kill=`ps -ef | grep "ceph-mon -f --cluster ${CEPH_CLUSTERNAME}" | grep -v grep | awk '{print $2}'`
        if [ -n "$pid_to_kill" ]; then
            kill -9 $pid_to_kill
            Log DEBUG "kill -9 $pid_to_kill"
        else
            Log WARN "not pid to kill, mybe service not running?"
        fi
    fi
    
    
    
}





######################################################################
# 作用: test MON service status
# 用法: Ceph_MON_Status_Service
# 注意：
######################################################################
Ceph_MON_Status_Service() {
    Log DEBUG "checking MON service status"
    if $CEPH --admin-daemon \
    /var/run/ceph/${CEPH_CLUSTERNAME}-mon.${MY_HOSTNAME}.asok mon_status >& /dev/null; then
        Log DEBUG "ceph MON service with clustername $CEPH_CLUSTERNAME is running..."
        return 0
    else
        Log WARN "ceph MON service with clustername $CEPH_CLUSTERNAME is not running.."
        return 1
    fi
}



######################################################################
# 作用: if installed ceph, prompt user
# 用法: Ceph_MON_Installed_Prompt
# 注意：
######################################################################
Ceph_MON_Installed_Prompt() {

    rm -rf ${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}*
    Log DEBUG "renew ceph cluster status from $MON_HOST"
    
    # remote mon host gen the cluster status file
    Remote_Exec "${MON_HOST}" "${BIN_DIR}/$(basename $0) -F 'Ceph_Cluster_Status'"

    # remote mon host push the cluster status file to ceph-ai
    Remote_Exec "${MON_HOST}" "${BIN_DIR}/$(basename $0) -F \"Pusher FILE ${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}\""
    
    # get the status file content, decided whether to pormpt warning
    if egrep -q "running|stopped" ${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}.*; then
        
        Log DEBUG "you are ready to install $CEPH_CLUSTERNAME with port ${CEPH_MON_PORT}, but ${COLOR_YELLOW}found the installed cluster information${COLOR_CLOSE}..."
        Log DEBUG ""
        divider='==================================='
        header="\n %10s %16s %12s %10s  %12s\n" 
        printf "$header" "HOSTNAME" "CLUSTERNAME" "PORT" "PID" "STATUS"
        
        echo "  $divider$divider"
        echo
        cat ${TMP_DIR}/${CEPH_CLUSTER_STATUS_FILE}.* | xargs -n5 | column -t | sed 's/^/   /' | egrep --color=always "noinstall|running|stoped"
        echo
        #echo "  $divider$divider"
        echo
        
        # pormpt warning
        Prompt_Yes_Or_No "Continue will cause ${COLOR_RED}DATA LOSE${COLOR_CLOSE} if you use the ${COLOR_RED}same clustername & port${COLOR_CLOSE}!!! \
        \n  You can edit ceph name & port in ${CONF_DIR}/setting and re-run\n  Do you realy realy want to continue?"
        Log DEBUG "choice:$ANS"
    else
        Log DEBUG "check mon status ready, continue..."
    fi 


}



######################################################################
# 作用: 生成Monmap
# 用法: Ceph_MON_Install_GenMonmap
# 注意：
######################################################################
Ceph_MON_Install_GenMonmap() {

    clusterid=`Ceph_UUID_Gen`
    Log DEBUG "gen UUID: $clusterid"

    # Create a keyring for the cluster and generate a secret key for the monitor
    if [ -f ${TMP_DIR}/${CEPH_CLUSTERNAME}.mon.keyring ]; then
        Log DEBUG "mon keyring for cluster $CEPH_CLUSTERNAME already exists, skip"
    else
        Log DEBUG "create mon keyring for cluster $CEPH_CLUSTERNAME"
        ceph-authtool --create-keyring ${TMP_DIR}/${CEPH_CLUSTERNAME}.mon.keyring --gen-key -n mon. --cap mon 'allow *' >&/dev/null
        Check_Return $0 $LINENO $FUNCNAME
    fi
    
    # Create an admin keyring, an admin user, and add the user to the keyring
    if [ -f ${TMP_DIR}/${CEPH_CLUSTERNAME}.client.admin.keyring ]; then
        Log DEBUG "client.admin keyring for cluster $CEPH_CLUSTERNAME already exists, skip"
    else
        Log DEBUG "create client.admin keyring for cluster $CEPH_CLUSTERNAME"
        ceph-authtool --create-keyring ${TMP_DIR}/${CEPH_CLUSTERNAME}.client.admin.keyring --gen-key -n client.admin \
        --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow' >&/dev/null
        Check_Return $0 $LINENO $FUNCNAME

    
        # Add the cleint.admin key to the ceph.mon.keyring
        Log DEBUG "add client.admin keyring to mon keyring"
        ceph-authtool ${TMP_DIR}/${CEPH_CLUSTERNAME}.mon.keyring --import-keyring ${TMP_DIR}/${CEPH_CLUSTERNAME}.client.admin.keyring >&/dev/null
        Check_Return $0 $LINENO $FUNCNAME
    fi
    
}


######################################################################
# 作用: Install MON, 可以指定不同的实例和不同的用户
# 用法: Ceph_MON_Install
# 注意：
######################################################################
Ceph_MON_Install() {

    which ceph-mon >&/dev/null || { Log DEBUG "installing ceph software" && yum -y -q install ceph; }
    
    ceph -v
    Check_Return  $0 $LINENO $FUNCNAME
    
    
    # create the user if not ceph
    # Ceph_User_Create
    
    # copy the ceph config file(already rsync after Ceph_Conf_Generater)
    #cp -rLfap ${RUN_DIR}/${CEPH_CONF_TEMPLATE_RENDERED_BY_MON} $CEPH_CONF
    #Log DEBUG "get the $CEPH_CONF"
    
    # change the ceph.conf, delete the {{for..end for}}
    sed -i '/{{for/,/end for}}/d' $CEPH_CONF
    
    
    chown -R ceph:ceph $CEPH_CONF
    
    # copy the client admin keyring
    cp -rLfap ${TMP_DIR}/${CEPH_CLUSTERNAME}.client.admin.keyring /etc/ceph/${CEPH_CLUSTERNAME}.client.admin.keyring
    chmod 644 /etc/ceph/${CEPH_CLUSTERNAME}.*

    # get the clusterid from ceph config 
    clusterid=`INI_Parser $CEPH_CONF global fsid`
    Log DEBUG "get the clusterid ${clusterid}"
    
    # Generate a monitor map using the monitor hostname, IP and fsid and save it as /tmp/monmap
    Log DEBUG "create monmap for cluster $CEPH_CLUSTERNAME"
    
    
    # get MON_INITIAL_HOST
    _MON_INITIAL_HOST=`INI_Parser $CEPH_CONF mon "mon initial members"`
    MON_INITIAL_HOST=(`echo $_MON_INITIAL_HOST | tr ',' ' ' | xargs`)
    
    _MON_IPADDR_PORT=`INI_Parser $CEPH_CONF mon "mon addr"`
    MON_IPADDR_PORT=(`echo $_MON_IPADDR_PORT | tr ',' ' ' | xargs`)
    
    # already add the "--add" to HOSTNAME_IP_PORT_LIST
    for (( i=0; i<${#MON_INITIAL_HOST[*]}; i++ )); do
        this_ipaddr=`echo ${MON_IPADDR_PORT[$i]} | awk -F':' '{print $1}'`
        this_port=`echo ${MON_IPADDR_PORT[$i]} | awk -F':' '{print $2}'`
        HOSTNAME_IP_PORT_LIST="$HOSTNAME_IP_PORT_LIST --add ${MON_INITIAL_HOST[$i]} $this_ipaddr:$this_port"
    done
    HOSTNAME_IP_PORT_LIST=`echo $HOSTNAME_IP_PORT_LIST`
    
    
    # add the all mon host to monmap
    Run monmaptool --create --clobber $HOSTNAME_IP_PORT_LIST \
    --fsid $clusterid ${TMP_DIR}/${CEPH_CLUSTERNAME}.monmap
    
    #Run monmaptool --create --clobber --add sc1r01n01 192.168.120.245:6789  --add sc1r02n02 192.168.120.244:6789 --add sc1r03n03 192.168.130.10:6789 \
    --fsid $clusterid ${TMP_DIR}/${CEPH_CLUSTERNAME}.monmap
    
    
    # Create a data directory
    if [ -d /var/lib/ceph/mon/${CEPH_CLUSTERNAME}-${MY_HOSTNAME} ]; then
        Log WARN "Already have a ceph MON with clustername=${CEPH_CLUSTERNAME}; now you decide to destroy it :("
        
        # stop service
        if Ceph_MON_Status_Service; then
            Ceph_MON_Stop_Service
        fi
        
        # remove service
        Ceph_MON_Remove_Service
        
        # rm dir
        Log DEBUG "Delete /var/lib/ceph/mon/${CEPH_CLUSTERNAME}-${MY_HOSTNAME}"
        rm -rf /var/lib/ceph/mon/${CEPH_CLUSTERNAME}-${MY_HOSTNAME}

    fi
    
    # chown & chmod ceph path
    Log DEBUG "Create the mon data dir /var/lib/ceph/mon/${CEPH_CLUSTERNAME}-${MY_HOSTNAME}"
    mkdir -p /var/lib/ceph/mon/${CEPH_CLUSTERNAME}-${MY_HOSTNAME}
    chown -R ${CEPH_USER}:${CEPH_GROUP} /var/lib/ceph
    chown -R ${CEPH_USER}:${CEPH_GROUP} /var/log/ceph
    chown -R ${CEPH_USER}:${CEPH_GROUP} /var/run/ceph
    chown -R ${CEPH_USER}:${CEPH_GROUP} /etc/ceph
    
    
    # Populate the monitor daemon with the monitor map and keyring
    Log DEBUG "create MON fs"
    Run ceph-mon --cluster  $CEPH_CLUSTERNAME --conf $CEPH_CONF --mkfs -i ${MY_HOSTNAME} \
    --monmap ${TMP_DIR}/${CEPH_CLUSTERNAME}.monmap --keyring ${TMP_DIR}/${CEPH_CLUSTERNAME}.mon.keyring \
    --setuser ${CEPH_UID} --setgroup ${CEPH_GID}
    

    

    touch /var/lib/ceph/mon/${CEPH_CLUSTERNAME}-${MY_HOSTNAME}/done
    
    # Create & start & check Service
    Ceph_MON_Create_Service
    Ceph_MON_Start_Service
    Ceph_MON_Status_Service
    Check_Return $0 $LINENO $FUNCNAME
}





######################################################################
# 作用: Uninstall the ceph mon with given name
# 用法: Ceph_Uninstall <clutername>
# 注意：
######################################################################
Ceph_MON_Uninstall() {
    
    # PreCheck
    # check the uninstall requirement
    
    
    # Confirm to uninstall
    
    
    # uninstall the MON with given clusteraname
    Log DEBUG "uinstall MON with cluster name $CEPH_CLUSTERNAME..."
    

    # stop the service
    Log DEBUG "stop MON service with cluster $CEPH_CLUSTERNAME..."
    if Ceph_MON_Status_Service; then
        Run Ceph_MON_Stop_Service
    fi
    
    # remove mon service
    Run Ceph_MON_Remove_Service

    # rm ceph MON dir
    Run rm -rf /var/lib/ceph/mon/${CEPH_CLUSTERNAME}-${MY_HOSTNAME}
    Run rm -rf /etc/ceph/${CEPH_CLUSTERNAME}.{conf,client.admin.keyring}
    
    
    # TODO: DO NOT REMOVE NOW! 
    # yum -y -q remove ceph ceph-release ceph-common ceph-radosgw
    
    
    Log SUCC "Uinstall mon with clustername $CEPH_CLUSTERNAME successful..."
    
    
    # Post install
    # 还原对系统所做的配置更改, 网卡使用changeDevice修改
    

}




######################################################################
# 作用: 检测并提示LVM/Mount的信息
# 用法: Ceph_OSD_Disk_Check
# 注意：检测到所有信息之后，退出前一起提示
######################################################################
Ceph_OSD_Disk_Check() {

    err_num=0
    warn_num=0
    
    # loop the OSD HOST
    for i in $OSD_HOST; do
        
        # get the sata disk(no root disk)
        disk_list=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_LIST`
        disk_size=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_SIZE`
        disk_sata=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_SATA`
        disk_ssd=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_SSD`
        disk_root=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_ROOT`
        disk_boot=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_BOOT`
        disk_inlvm=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_INLVM_RAID`
        disk_mounted=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_MOUNTED`

        # debug
        Log DEBUG "[$i] All disk are: $disk_list"
        Log DEBUG "[$i] SATA disk are: $disk_sata"
        Log DEBUG "[$i] SSD disk are: $disk_ssd"
        Log DEBUG "[$i] ROOT disk are: $disk_root"
        Log DEBUG "[$i] BOOT disk are: $disk_boot"
        Log DEBUG "[$i] LVM disk are: $disk_inlvm"
        Log DEBUG "[$i] Mounted disk are: $disk_mounted"
        
        # base check
        [ -z "$disk_list" ] && Log ERROR "disk_list is null, please contact system master" "no_exit"
        [ -z "$disk_root" ] && Log ERROR "disk_root is null, please contact system master" "no_exit"
        
        
        # exclude disk_root & disk_boot
        if [ -n "$disk_boot" ]; then
            disk_sata=`echo $disk_sata | sed "s/$disk_root//" | sed "s/$disk_boot//" | xargs`
            disk_ssd=`echo $disk_ssd | sed "s/$disk_root//" | sed "s/$disk_boot//" | xargs`
            disk_mounted=`echo $disk_mounted | sed "s/$disk_root//" | sed "s/$disk_boot//" | xargs`
            
        else
            disk_sata=`echo $disk_sata | sed "s/$disk_root//" | xargs`
            disk_ssd=`echo $disk_ssd | sed "s/$disk_root//" | xargs`
            disk_mounted=`echo $disk_mounted | sed "s/$disk_root//" | xargs`
            
        fi
        
        # if there is no sata disk & ssd , error
        if [ -z "$disk_sata" -a -z "$disk_ssd" ]; then
            Log ERROR "[$i]: no sata/sas/ssd disk found(exclude root/boot disk)" "no_exit"
            err_num=`expr $err_num + 1`

        fi

        
        # if found disk in lvm(exclude root disk/boot disk), error
        for j in $disk_inlvm; do
            if echo $disk_sata $disk_ssd | grep -q $j ; then
                Log ERROR "[$i]: disk $j in lvm, please detach it or reinstall os!!" "no_exit"
                err_num=`expr $err_num + 1`
                #Ask_Y_Or_N ""
                
            fi
        done
        
        
        
        # if find mounted disk(not root disk), warn
        if [ -n "`echo $disk_mounted`" ]; then
            Log WARN "[$i]: Find mounted volume, will umount force later"
            warn_num=`expr $warn_num + 1`
        fi
        
        
        # if there is no ssd, warn
        if [ -z "$disk_ssd" ]; then
            Log WARN "[$i]: no ssd found, i will use the sata disk -> part2($CEPH_OSD_JOURNAL_SIZE per disk) as journal"
            warn_num=`expr $warn_num + 1`

        fi
        
        # if there is no sata, good
        if [ -z "$disk_sata" ]; then
            Log DEBUG "[$i]: oh, all ssds, wonderful, you are in a good company"
        else
            # if sata disk count < $CEPH_OSD_DISK_MIN, warn
            if [ $(echo $disk_sata | wc -w) -lt $CEPH_OSD_DISK_MIN ]; then
                Log WARN "[$i]: at least $CEPH_OSD_DISK_MIN osd disk is recommand"
                warn_num=`expr $warn_num + 1`
            fi
        fi
        
        
        # if each disk size less than $CEPH_OSD_JOURNAL_SIZE, error
        for i in `echo $DISK_SIZE | xargs -n 1 | awk -F: '{print $2}'`; do
            if [ $i -lt $CEPH_OSD_JOURNAL_SIZE ]; then
                Log ERROR "[$i]: There is a disk with only ${i}GB space, at least $CEPH_OSD_JOURNAL_SIZE required" "no_exit"
                err_num=`expr $err_num + 1`
            fi
        done
        echo
        
    done
    
    
    
    
    # if there is error or warnning, pormpt info
    if [ "$warn_num" -gt 0 -o "$err_num" -gt 0 ]; then
        Log DEBUG "Find $err_num error(s), $warn_num warinning(s)"
    fi
        
    
    # if there is error, exit now
    if [ "$err_num" -gt 0 ]; then
        Log ERROR " Tips:\
        \n  > DO NOT use lvm(exclude root disk) \
        \n  > Disk space must be bigger than $CEPH_OSD_JOURNAL_SIZE GB \
        \n  > umount all of the disk is recommand(exclude root disk) \
        \n  > ssd is recommand for journal \
        \n  > at least $CEPH_OSD_DISK_MIN osds per node is recommand \
        \n  Please resolve the error above and run again!!"
    fi
    
    # yes to continue
    Ask_Y_Or_N "continue will force umount and format the osd disk, \
    \n  If the disk you DO NOT want to use , edit ${UNDERLINE}${RUN_DIR}/$CEPH_OSD_MANUAL_CONF${NORMAL}
    \n  Ready to continue?"
    

    
    # diff -W 130 ../run/hostinfo.pusher.sc1r01n01 ../run/hostinfo.pusher.sc1r02n02 --side-by-side
}





######################################################################
# 作用: 根据每个osd节点磁盘信息, 结合用户定义文件，计算出osd序列
# ++ 并将结果存为: ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE}
# 用法: Ceph_OSD_AutoAssign
# 注意：
######################################################################
Ceph_OSD_Assign() {
    
    first_num=0
    >${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE}
    
    Log DEBUG "Gen global osd assign file to ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE}"
    Log DEBUG ""
    
    # loop the OSD HOST
    for i in $OSD_HOST; do        

        # get the sata disk(no root disk)
        disk_list=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_LIST`
        disk_sata=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_SATA`
        disk_ssd=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_SSD`
        disk_root=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_ROOT`
        disk_boot=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_BOOT`
        disk_path=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_PATH`
        disk_size=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE}.$i disk DISK_SIZE`
        
        # exclude disk_root & disk_boot
        if [ -n "$disk_boot" ]; then
            disk_sata=`echo $disk_sata | sed "s/$disk_root//" | sed "s/$disk_boot//" | xargs`
            disk_ssd=`echo $disk_ssd | sed "s/$disk_root//" | sed "s/$disk_boot//" | xargs`
            
        else
            disk_sata=`echo $disk_sata | sed "s/$disk_root//" | xargs`
            disk_ssd=`echo $disk_ssd | sed "s/$disk_root//" | xargs`
            
        fi
        
        # if only ssd or only sata, use disk(exclude sata root disk) as ceph data/journal osd
        disk_data=$disk_sata
        disk_journal=$disk_ssd
        
        [ -z "$disk_sata" ] && disk_data=$disk_ssd
        [ -z "$disk_ssd" ]  && disk_journal=$disk_sata

        
        Log DEBUG "[$i] auto calc disk_data=\"$disk_data\""
        Log DEBUG "[$i] auto calc disk_journal=\"$disk_journal\""


        
        # second: if user define data & journal
        if [ -f "${RUN_DIR}/$CEPH_OSD_MANUAL_CONF" ]; then
            user_define_disk_data=`INI_Parser ${RUN_DIR}/${CEPH_OSD_MANUAL_CONF} ${i} disk_data`
            user_define_disk_journal=`INI_Parser ${RUN_DIR}/${CEPH_OSD_MANUAL_CONF} ${i} disk_journal`
            user_define_disk_exclude=`INI_Parser ${RUN_DIR}/${CEPH_OSD_MANUAL_CONF} ${i} disk_exclude`
            user_define_disk_primary_affinity=`INI_Parser ${RUN_DIR}/${CEPH_OSD_MANUAL_CONF} ${i} disk_primary_affinity`
                        
            # use user_define_data
            if [ -n "$user_define_disk_data" ]; then
                Log WARN "[$i] found user define data disk in ${RUN_DIR}/$CEPH_OSD_MANUAL_CONF, data disk: $user_define_disk_data"
                user_define_data=true
                
                # overwrite the existing disk_data
                disk_data=`echo $user_define_disk_data | xargs -n1 | sort -u | xargs`
            else
                user_define_data=false
                Log DEBUG "[$i] ok, i will use auto assign data disk and coutinue"
            fi
            
            # use user_define_journal
            if [ -n "$user_define_disk_journal" ]; then
                Log WARN "[$i] found user define journal disk in ${RUN_DIR}/$CEPH_OSD_MANUAL_CONF, journal disk: $user_define_disk_journal"
                user_define_journal=true
                disk_journal=`echo $user_define_disk_journal | xargs -n1 | sort -u | xargs`
            else
                user_define_journal=false
                Log DEBUG "[$i] ok, i will use auto assign journal disk and coutinue"
            fi
            
            

            
            
            # delete the exclude disk
            if [ -n "$user_define_disk_exclude" ]; then
                Log WARN "[$i] found user define exclude disk in ${RUN_DIR}/$CEPH_OSD_MANUAL_CONF, exclude disk: $user_define_disk_exclude"
                user_define_exclude=true
                for j in $user_define_disk_exclude; do 
                    disk_data=`echo $disk_data | sed "s/$j//"`
                    disk_journal=`echo $disk_journal | sed "s/$j//"`
                done
                
                disk_data=`echo $disk_data`
                disk_journal=`echo $disk_journal`
                
            else
                user_define_exclude=false
            fi
            

        fi
        

        # if no data or journal, error
        [ -z "$disk_data" -o -z "$disk_journal" ] && Log ERROR "[$i] data or journal disk can not be null"
        
        # check: if data disk are not in disk_list
        for j in $disk_data; do
            # if not in disk_list
            if ! echo $disk_list | grep -qw $j; then
                Log ERROR "[$i] $j not in disk_list, if you are sure disk exists, that means system do not recognize disk $j"
            fi
        done
        
        # check: if journal disk are not in disk_list
        for j in $disk_journal; do
            # if not in disk_list
            if ! echo $disk_list | grep -qw $j; then
                Log ERROR "[$i] $j not in disk_list, if you are sure disk exists, that means system do not recognize disk $j"
            fi
        done
        
        

        # check: Check Assign logical & user define logical
        #1. first:  if data disk = journal disk(share_disk mode)
        #2. second: data disk & journal disk is all difference(standalone mode)
        if [ "$disk_data" = "$disk_journal" ]; then
            diskmode=shared
        else
            
            for j in $disk_data; do
            
                # if disk_data & data journal has intersection
                if echo $disk_journal | grep -qw $j ; then
                    Log ERROR "This disk mode not support \
                    \n  Only support the follow two disk mode: \
                    \n    1. Shared: data disk & journal disk are all the same \
                    \n    2. Standalone: data disk & journal disk are all difference
                    \n  For more information, please contact system administrator"
                fi
            done
            diskmode=standalone
        fi
        
        
        
        
        # disk mode will affect the disk_primary_affinity set
        if [ -f "${RUN_DIR}/$CEPH_OSD_MANUAL_CONF" ]; then
            # if disk_primary_affinity is set
            if [ -n "$user_define_disk_primary_affinity" ]; then
                Log WARN "[$i] found user define primary affinity disk in ${RUN_DIR}/$CEPH_OSD_MANUAL_CONF, primary disk: $user_define_disk_primary_affinity"
                user_define_disk_primary=true
                for j in $user_define_disk_primary_affinity; do
                    # shared mode, add to the data & journal disk
                    if [ "$diskmode" = "shared" ]; then
                        if echo $disk_data | grep -vq $j; then
                            disk_data="$disk_data $j"
                            Log DEBUG "[$i] add disk $j to data disk list, it will be use as primary osd later"
                        fi
                        if echo $disk_journal | grep -vq $j; then
                            disk_data="$disk_data $j"
                            Log DEBUG "[$i] add disk $j to journal disk list"
                        fi
                    else
                        # only add to disk_data
                        disk_data="$disk_data $j"
                        Log DEBUG "[$i] add disk $j to data disk only, it will be use as primary osd later"
                    fi
                done
                
                disk_data=`echo $disk_data`
                disk_journal=`echo $disk_journal`
                
            else
                user_define_disk_primary=false
            fi
        fi
        
        
        Log DEBUG "[$i] disk_data=\"$disk_data\""
        Log DEBUG "[$i] disk_journal=\"$disk_journal\""
        
        # get data disk num
        osd_data_disk_num=`echo $disk_data | wc -w`
        osd_journal_disk_num=`echo $disk_journal | wc -w`
        lastnum=`expr $first_num + $osd_data_disk_num - 1`
        
        if [ "$diskmode" = "standalone" ]; then
            if [ "$osd_journal_disk_num" -gt 1 ]; then
            
                # use the small/fastest disk to be journal disk; other disk will add to auto_disk_primary
                Log DEBUG "found $osd_journal_disk_num ssds, the smallest ssd will use as journal disk"
                ssd_for_egrep=`echo $disk_journal | sed 's/ /|/g'`
                ssdname_size_list=`echo $DISK_SIZE | xargs -n1 | egrep "$ssd_for_egrep"`
                
                # if the disk size is not the same, use the smallest as journal
                if [ `echo "$ssdname_size_list" | awk -F ':' '{print $2}' | sort -u | wc -l` -gt 1 ]; then
                    smallest_ssd_sorted=`echo "$ssdname_size_list" | sort -t : -k2 -n`
                    
                    
                    smallest_ssd=`echo "$ssdname_size_list" | sort -t : -k2 -n | sed -n '1p' | awk -F':' '{print $1}'`
                    
                    Log DEBUG "set journal disk to the smallest disk $smallest_ssd"
                    disk_journal=$smallest_ssd
                    
                    # TODO: other disk auto add to primary osd?
                    
                    
                else
                
                    Log DEBUG "the ssd size are the same, use speed test"
                    # use fio to test the iops, the fastest use as journal disk, other use as primary osd(if they are not in "user define disks")
                    unset ssd_iops_list
                    for s in $disk_journal; do
                        Log -n DEBUG "test about 10 seconds wirte iops for $s on $i..."
                        iops_command="fio -filename=/dev/$s -direct=1 -iodepth 8 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k \
                        -size=10G -numjobs=4 -runtime=10 -group_reporting -name=mytest -ioscheduler=noop"
                        iops_result=`ssh -o StrictHostKeyChecking=no -q $i "$iops_command"`
                        iops=`echo "$iops_result" | grep iops | grep write | sed 's/.*iops=\(.*\),.*/\1/'`
                        Log DEBUG "write iops=$iops"
                        
                        ssd_iops_list="$ssd_iops_list $s:$iops"
                    done
                    
                    # get the FASTEST iops
                    fastest_iops_ssd=`echo "$ssd_iops_list" | xargs -n1 | sort -t : -k2 -n | sed -n '$p' | awk -F':' '{print $1}'`
                    
                    Log DEBUG "set journal disk to the fastest disk: $fastest_iops_ssd"
                    disk_journal=$fastest_iops_ssd
                    
                fi
            fi
            
            # check the disk_journal size, must be bigger than $osd_data_disk_num * $CEPH_OSD_JOURNAL_SIZE
            #  use lsblk to get the true capacity
            
            # get the disk_journal size
            journal_size=`echo $disk_size | xargs -n 1 | grep -w $disk_journal | awk -F: '{print $2}'`
            Log DEBUG "[$i] journal_size=$journal_size"
            
            # required size
            let required_journal_size=$osd_data_disk_num*$CEPH_OSD_JOURNAL_SIZE
            
            if [ $journal_size -lt $required_journal_size ]; then
                Log ERROR "[$i] $disk_journal size required: $required_journal_size, but it only has $journal_size"
            fi
            
            Log DEBUG "[$i] check journal capacity passed"
            
        fi
        
        Log DEBUG ""

        # loop to file
        cat <<EOF >>${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE}
[$i]
    osd_number = $first_num-$lastnum
    osd_data_disk = "$disk_data"
    osd_journal_disk = "$disk_journal"
    
    diskmode = $diskmode
    
    user_define_data = $user_define_data
    user_define_journal = $user_define_journal
    user_define_exclude = $user_define_exclude
    user_define_disk_primary = $user_define_disk_primary
    
    auto_disk_primary = "$auto_disk_primary"

EOF
        
        first_num=`expr $lastnum + 1`
    done

        # prepare for adding more osds
        cat <<EOF >>${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE}
[add-more-osd]
    osd_number_start_next = `expr $lastnum + 1`
EOF

   
    
}



######################################################################
# 作用: umount, format, mount
# 用法: Ceph_OSD_Disk_Prepare
# 注意：
######################################################################
Ceph_OSD_Disk_Prepare() {
   
    disk_data=`INI_Parser ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE} ${MY_HOSTNAME} osd_data_disk`
    disk_journal=`INI_Parser ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE} ${MY_HOSTNAME} osd_journal_disk`
    diskmode=`INI_Parser ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE} ${MY_HOSTNAME} diskmode`
    osd_number=`INI_Parser ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE} ${MY_HOSTNAME} osd_number`
    disk_path=`INI_Parser ${RUN_DIR}/${HOSTINFO_FILE} disk DISK_PATH`
    
    # if already has ceph, stop it
    Ceph_OSD_Status_Service
    if [ $? -eq 0 -o $? -eq 10 ]; then
        Run Ceph_OSD_Stop_Service
    fi
    
    # umount disk
    for i in $disk_data $disk_journal; do
        # if has this disk mount info
        if mount | grep -q $i; then
            # get the mount point(s)
            Log DEBUG "ready to umount $i partition"
            mount_point=`mount | grep $i | awk '{print $3}'`
            
            for j in $mount_point; do
                if umount -f $j; then
                    Log DEBUG "umount $j success"
                    
                    if grep -q $j /etc/fstab; then
                        Log DEBUG "remove automount $j in /etc/fstab"
                        mount_point_escape=$(echo $j | sed 's/\//\\\//g')
                        sed -i "/$mount_point_escape/d" /etc/fstab
                    fi
                else
                
                    # change dir to umount
                    cd ~
                    Run_More umount -f $j
                    cd -
                fi
            done
        else
            Log DEBUG "$i not mount, good, skip"
        fi
    done
    Log DEBUG ""
    
    # change disk_data, disk_journal to device path
    for i in $disk_path; do
        for j in $disk_data; do
            if echo $i | grep -q $j; then
                disk_data_device="$disk_data_device $i"
            fi
        done
    done
    for i in $disk_path; do
        for j in $disk_journal; do
            if echo $i | grep -q $j; then
                disk_journal_device="$disk_journal_device $i"
            fi
        done
    done
    disk_data_device=`echo $disk_data_device`
    disk_journal_device=`echo $disk_journal_device`
    Log DEBUG "disk_data_device=$disk_data_device"
    Log DEBUG "disk_journal_device=$disk_journal_device"
    
    # part/format/mount data disk
    # import the external library to format
    . ${LIB_DIR}/disk/disk_format.sh
    
    # if shared disk, part1 data, part2 journal
    Log DEBUG ""
    Log DEBUG "ready to format $diskmode mode disk"
    Disk_Part_Format_Mount "${disk_data_device}" "${disk_journal_device}" "${CEPH_OSD_FSTYPE}" "${osd_number}" "${CEPH_OSD_MKFS_OPTION}" \
    "${CEPH_OSD_MKFS_LOGDEV_SIZE}"  "${CEPH_OSD_MOUNT_OPTION}"  "${CEPH_USER}:${CEPH_GROUP}"  "${CEPH_OSD_JOURNAL_SIZE}"  "${diskmode}"

}


######################################################################
# 作用: install OSD
# 用法: Ceph_OSD_Install
# 注意：
######################################################################
Ceph_OSD_Install() {

    which ceph-osd >&/dev/null || { Log DEBUG "installing ceph software" && yum -y -q install ceph; }
    
    Log DEBUG "OSD install depend on MON, checking ceph MON status first"
    if ! Ceph_MON_Status_Service; then
        Ceph_MON_Start_Service
        if ! Ceph_MON_Status_Service; then
            Log ERROR "Mon status NOT ok"
        fi
    fi
    Log DEBUG "ceph MON status ok"
    
    
    # change the ceph.conf, delete the {{for..end for}}
    sed -i '/{{for/,/end for}}/d' $CEPH_CONF
    
    # already copyed by Rsync_File
    chown -R ceph:ceph $CEPH_CONF
    
    
    # get my osd_number 
    osd_number=`INI_Parser ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE} ${MY_HOSTNAME} osd_number`
    osd_data_disk=`INI_Parser ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE} ${MY_HOSTNAME} osd_data_disk`
    osd_number_begin=`echo $osd_number | awk -F'-' '{print $1}'`
    osd_number_end=`echo $osd_number | awk -F'-' '{print $2}'`
    
    
    ## env prepare
    # stop service
    Ceph_OSD_Status_Service
    if [ $? -eq 0 -o $? -eq 10 ]; then
        Run Ceph_OSD_Stop_Service
    fi

    # remove service
    Run Ceph_OSD_Remove_Service

    
    # remove existing osds
    for i in `$CEPH osd ls`; do
        Run $CEPH osd crush remove osd.$i
        Run $CEPH auth del osd.$i
        
        Run $CEPH osd down $i
        Run $CEPH osd rm $i
        Log DEBUG ""
    done
    
    # remove hosts(if not empty, will remove failed)
    Run $CEPH osd crush remove ${MY_HOSTNAME}
    Log DEBUG ""
    
    for ((i=$osd_number_begin; i<=$osd_number_end; i++)); do
    
        # create ceph osd 
        Run $CEPH osd create $i
        
        # initialize the OSD data directory
        Run_More ceph-osd --cluster $CEPH_CLUSTERNAME -i $i --mkfs --mkkey
        
        # remove the existing ceph auth key 
        if $CEPH auth get osd.${i} >&/dev/null; then
            Run $CEPH auth del osd.${i}
        fi
        
        # register the OSD authentication key
        if $CEPH auth add osd.$i osd 'allow *' mon 'allow profile osd' -i \
        /var/lib/ceph/osd/${CEPH_CLUSTERNAME}-${i}/keyring >&/dev/null; then
            Log DEBUG "$CEPH auth add osd.$i"
        else
            Log ERROR "add auth osd.${i} failed"
        fi
        Log DEBUG ""
    done
    
    # Add OSD node to CRUSH map
    # ceph osd crush add-bucket <name> <type>
    Run $CEPH osd crush add-bucket ${MY_HOSTNAME} host
    
    # Place the OSD node under root by default
    # osd crush move <name> <args>
    Run $CEPH osd crush move ${MY_HOSTNAME} root=default
    
    # Add the OSD node to the CRUSH map for receiving data 
    # ceph osd crush add {id-or-name} {weight} [{bucket-type}={bucket-name} ...]
    # TODO: weight calc: capacity, speed
    

    for ((i=$osd_number_begin; i<=$osd_number_end; i++)); do
        Run $CEPH osd crush add osd.${i} 1.0 host=${MY_HOSTNAME}
        
        # change permission
        Run chown -R ceph:ceph /var/lib/ceph/osd/ceph-${i}/
    done
    Log DEBUG ""
    
    # Create & start & check Service
    Ceph_OSD_Create_Service
    Ceph_OSD_Start_Service
    
    # DO NOT check now!!! (osd start is not synchronizatin)
    #Ceph_OSD_Status_Service
    #Check_Return $0 $LINENO $FUNCNAME

    ## ceph osd postconf 
    # TODO: set osd primary-affinity, how to set weight?
    if [ -n "$user_define_disk_primary" ] ; then
        :
    #Run ceph --cluster $CEPH_CLUSTERNAME osd primary-affinity <osd-id> <weight>
    fi
    
    
    ## make rbd replicate size in setting.conf
    exist_pool=`$CEPH osd pool ls`
    OSD_POOL_PG_NUM=`INI_Parser ${CEPH_CONF} global "osd pool default pg num"`
    OSD_POOL_PGP_NUM=`INI_Parser ${CEPH_CONF} global "osd pool default pgp num"`
    ceph_status=`$CEPH -s`
    
    Log DEBUG "apply osd post install setting"
    for i in $exist_pool; do
    
        # set the pool size
        act_size=`$CEPH osd pool get $i size | awk '{print $2}'`
        if [ $act_size -ne "$CEPH_OSD_POOL_DEFAULT_SIZE" ]; then
            Run $CEPH osd pool set $i size $CEPH_OSD_POOL_DEFAULT_SIZE
        fi
        
        # set the pool min_size
        act_min_size=`$CEPH osd pool get $i min_size | awk '{print $2}'`
        if [ $act_min_size -ne "$CEPH_OSD_POOL_DEFAULT_MIN_SIZE" ]; then
            Run $CEPH osd pool set $i min_size $CEPH_OSD_POOL_DEFAULT_MIN_SIZE
        fi
        
        act_pg_num=`$CEPH osd pool get $i pg_num | awk '{print $2}'`
        act_pgp_num=`$CEPH osd pool get $i pgp_num | awk '{print $2}'`
        
        
        # if actual pg & "pg in ceph conf" not the same
        if [ $act_pg_num -ne $OSD_POOL_PG_NUM ]; then
        
            # if set failed
            if ! $CEPH osd pool set $i pg_num $OSD_POOL_PG_NUM; then
                
                # if too few PGs
                if echo "$ceph_status" | grep -q "too few PGs per OSD"; then
                    # set pg_num & pgp_num to at_least_pg_required
                    at_least_pg_required_per_osd=`echo "$ceph_status" | grep "too few PGs per OSD" | sed 's/.* min \(.*\))/\1/'`
                    OSD_NUMS=`INI_Parser ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE} add-more-osd osd_number_start_next`
                    
                    let at_least_pg_required=$at_least_pg_required_per_osd*$OSD_NUMS  
                    
                    Log DEBUG "set pg_num/pgp_num to min request $at_least_pg_required"
                    Run $CEPH osd pool set $i pg_num  $at_least_pg_required
                    Run $CEPH osd pool set $i pgp_num $at_least_pg_required
                fi
                
            
            fi
        fi
       
        
    done
    

}







######################################################################
# 作用: re-generate systemd for osd service, use base ceph-osd@service
# 用法: Ceph_OSD_Create_Service
# 注意：
######################################################################
Ceph_OSD_Create_Service() {
    
    Log DEBUG "creating OSD service file"
    system_service_dir="/usr/lib/systemd/system"
    
    # src is default ceph
    src_osd_target="ceph-osd.target"
    src_osd_service_instance="ceph-osd@.service"

    # dst is ${CEPH_CLUSTERNAME}
    dst_osd_target="${CEPH_CLUSTERNAME}-osd.target"
    dst_osd_service_instance="${CEPH_CLUSTERNAME}-osd@.service"
    
    # copy from src to dst(if CEPH_CLUSTERNAME is not ceph)
    if [ "$src_osd_target" != "$dst_osd_target" ]; then
        cp -rLfap ${system_service_dir}/${src_osd_target} ${system_service_dir}/${dst_osd_target}
    fi
    if [ "$src_osd_service_instance" != "$dst_osd_service_instance" ]; then
        cp -rLfap ${system_service_dir}/${src_osd_service_instance} ${system_service_dir}/${dst_osd_service_instance}
    fi
    
    src_partof="ceph-osd.target"
    dst_partof="${CEPH_CLUSTERNAME}-osd.target"
    src_wantby="ceph-osd.target"
    dst_wantby="${CEPH_CLUSTERNAME}-osd.target"

    # replace the "Environment=CLUSTER=ceph"
    sed -i "s/Environment=CLUSTER=ceph/Environment=CLUSTER=${CEPH_CLUSTERNAME}/" \
    ${system_service_dir}/${dst_osd_service_instance}
    
    # replace the "--setuser ceph --setgroup ceph"
    sed -i "s/--setuser \(.*\) --setgroup \(.*\)/--setuser $CEPH_USER --setgroup ${CEPH_GROUP}/" \
    ${system_service_dir}/${dst_osd_service_instance}
    
    # replate the PartOf & WantedBy
    sed -i "s/PartOf=$src_partof/PartOf=$dst_partof/" ${system_service_dir}/${dst_osd_service_instance}
    sed -i "s/WantedBy=$src_wantby/WantedBy=$dst_wantby/" ${system_service_dir}/${dst_osd_service_instance}

    # auto start the service
    Run systemctl enable ceph.target
    Run systemctl enable ${CEPH_CLUSTERNAME}-osd.target
    
    
    ## enable osd daemon
    # get my osd_number 
    osd_number=`INI_Parser ${RUN_DIR}/${CEPH_OSD_ASSIGN_FILE} ${MY_HOSTNAME} osd_number`
    osd_number_begin=`echo $osd_number | awk -F'-' '{print $1}'`
    osd_number_end=`echo $osd_number | awk -F'-' '{print $2}'`
    for ((i=$osd_number_begin; i<=$osd_number_end; i++)); do
        Run systemctl enable ${CEPH_CLUSTERNAME}-osd@${i}
    done
    
    
    Check_Return  $0 $LINENO $FUNCNAME
    Log SUCC "create OSD service file successful"
    
    
}


######################################################################
# 作用: remove systemd service for OSD service
# 用法: Ceph_OSD_Remove_Service
# 注意：
######################################################################
Ceph_OSD_Remove_Service() {

    # disable the service
    Log DEBUG "remove OSD service instance for cluster $CEPH_CLUSTERNAME"
    
    for i in `systemctl --plain | grep ${CEPH_CLUSTERNAME}-osd@[0-9] | awk '{print $1}'`; do
        Run systemctl disable $i
    done
    
    # flush the service instance
    Run systemctl reset-failed
    
    # disable the osds
    Run systemctl disable ${CEPH_CLUSTERNAME}-osd.target
    
    Log DEBUG "remove OSD service file for cluster $CEPH_CLUSTERNAME"
    system_service_dir="/usr/lib/systemd/system"

    # dst is ${CEPH_CLUSTERNAME}
    dst_osd_target="${CEPH_CLUSTERNAME}-osd.target"
    dst_osd_service_instance="${CEPH_CLUSTERNAME}-osd@.service"

    # if clustername is ceph, do not delete the service file
    if [ "$CEPH_CLUSTERNAME" != "ceph" ]; then
        if [ -f "${system_service_dir}/${dst_osd_target}" ]; then
            Run rm -rf ${system_service_dir}/${dst_osd_target}
        else
            Log DEBUG "${system_service_dir}/${dst_osd_target} does not exist"
        fi
        
        if [ -f "${system_service_dir}/${dst_osd_service_instance}" ]; then
            Run rm -rf ${system_service_dir}/${dst_osd_service_instance}
        else
            Log DEBUG "${system_service_dir}/${dst_osd_service_instance} does not exist"
        fi
    else
        Log DEBUG "clustername is $CEPH_CLUSTERNAME, no need to delete"
    fi
    
    Log SUCC "remove OSD service success"
    
    
}




######################################################################
# 作用: start systemd OSD service
# 用法: Ceph_OSD_Start_Service
# 注意：
######################################################################
Ceph_OSD_Start_Service() {
    Run systemctl start ${CEPH_CLUSTERNAME}-osd.target
    sleep 5
}




######################################################################
# 作用: stop systemd OSD service
# 用法: Ceph_OSD_Stop_Service
# 注意：
######################################################################
Ceph_OSD_Stop_Service() {

    # set osd noout
    # ceph osd set   noout
    # ceph osd unset noout

    if systemctl stop ${CEPH_CLUSTERNAME}-osd.target; then
        sleep 5
        Log SUCC "stop OSD service successful"
    else
        Log WARN "systemctl stop ${CEPH_CLUSTERNAME}-osd.target failed, force kill"
        pid_to_kill=`ps -ef | grep "ceph-osd -f --cluster ${CEPH_CLUSTERNAME}" | grep -v grep | awk '{print $2}'`
        if [ -n "$pid_to_kill" ]; then
            kill -9 $pid_to_kill
            Log DEBUG "kill -9 $pid_to_kill"
        else
            Log WARN "not pid to kill, mybe service not running?"
        fi
    fi
    
    
    
}





######################################################################
# 作用: test OSD service status
# 用法: Ceph_OSD_Status_Service
# 注意：all running return0, some running return 10, no config return 1
######################################################################
Ceph_OSD_Status_Service() {
    Log DEBUG "checking OSD status with \"$CEPH osd stat\""
    
    # use ceph osd stat
    osd_stat=`$CEPH osd stat`
    total_osd=`echo "$osd_stat" | grep osdmap | awk '{print $3}'`
    up_osd=`echo "$osd_stat" | grep osdmap | awk '{print $5}'`
    
    [ -z "$total_osd" ] && Log WARN "ceph OSD daemon not config" && return 1
    [ -z "$up_osd" ] && Log WARN "ceph OSD not running" && return 1    
    
    if [ "$total_osd" -gt 0 ]; then
        if [ "$up_osd" -eq  "$total_osd" ]; then
            Log DEBUG "ceph OSD daemon are all running"
            return 0
        else
            Log WARN "some ceph OSD daemon running, some not"
            return 10
        fi
    fi
}






######################################################################
# 作用: check "ceph -s"
# 用法: Ceph_Status
# 注意：all running return0, some running return 10, no config return 1
######################################################################
Ceph_Status() {

    ceph_status=`$CEPH -s`

    Log DEBUG "check ceph status..."
    
    # if health ok
    if echo "$ceph_status" | grep -q "HEALTH_OK"; then
        return 0

    # if health ERR
    elif echo "$ceph_status" | grep -q "HEALTH_ERR"; then
        return 1
    
    # if health WARN
    elif echo "$ceph_status" | grep -q "HEALTH_WARN"; then
        return 10
        
    fi
}


######################################################################
# 作用: check the ceph status
# 用法: Ceph_Check_Status
# 注意：
######################################################################
Ceph_Check_Status() {

    # check ceph osd status
    Ceph_OSD_Status_Service

    # Check ceph status
    Ceph_Status
    Check_Return $0 $LINENO $FUNCNAME
}







######################################################################
# 作用: Uninstall the ceph mon with given name
# 用法: Ceph_Uninstall <clutername>
# 注意：
######################################################################
Ceph_OSD_Uninstall() {

    
    # PreCheck
    # check the uninstall requirement
    
    
    # Confirm to uninstall
    

    
    # uninstall the mon with given clusteraname
    Log DEBUG "uinstall OSD with cluster name $CEPH_CLUSTERNAME..."
    
    
    # check mount & /etc/fstab 
    
    ## Remove OSDs
    #ceph osd crush remove osd.{osd-num}
    #ceph auth del osd.{osd-num}
    #ceph osd rm {osd-num}
    #ceph osd crush remove {host}

    # stop the service
    Ceph_OSD_Status_Service
    if [ $? -eq 0 -o $? -eq 10 ]; then
        Run Ceph_OSD_Stop_Service
    fi
    
    # remove OSD service
    Run Ceph_OSD_Remove_Service
    
    
    # 
    # remove existing osds
    
    for i in `$CEPH osd ls`; do
        Run $CEPH osd crush remove osd.$i
        Run $CEPH auth del osd.$i
        
        Run $CEPH osd down $i
        Run $CEPH osd rm $i
        Log DEBUG ""
    done
    
    # remove hosts(if not empty, will remove failed)
    Run $CEPH osd crush remove ${MY_HOSTNAME}
    Log DEBUG ""
    
    for ((i=$osd_number_begin; i<=$osd_number_end; i++)); do

        # remove the existing ceph auth key 
        if $CEPH auth get osd.${i} >&/dev/null; then
            Run $CEPH auth del osd.${i}
        fi
        
        # rm ceph OSD dir
        Run rm -rf /var/lib/ceph/osd/${CEPH_CLUSTERNAME}-*
    done

    
    
    
    
    
    # TODO: DO NOT REMOVE NOW! 
    # yum -y -q remove ceph ceph-release ceph-common ceph-radosgw
     
    
    Log SUCC "Uinstall OSD with clustername $CEPH_CLUSTERNAME successful..."
    
    
    # Post install
    # 还原对系统所做的配置更改, 网卡使用changeDevice修改
    

}


######################################################################
# 作用: Install CEPH MDS 
# 用法: Ceph_MDS_Install
# 注意：
######################################################################
Ceph_MDS_Install() {

    which ceph-mds >&/dev/null || { Log DEBUG "installing ceph software" && yum -y -q install ceph; }
    ceph -v
    Check_Return  $0 $LINENO $FUNCNAME
    
    Log DEBUG "install Ceph MDS"
    chown -R ceph:ceph $CEPH_CONF
    
    # Create data directory
    if [ -d /var/lib/ceph/mds/mds-${MY_HOSTNAME} ]; then
        Log WARN "Already have a ceph MDS with clustername=${CEPH_CLUSTERNAME}; now you decide to destroy it :("
        
        # stop service
        if Ceph_MDS_Status_Service; then
            Ceph_MDS_Stop_Service
        fi
        
        # remove service
        Ceph_MDS_Remove_Service
        
        # rm dir
        Log DEBUG "Delete /var/lib/ceph/mds/mds-${MY_HOSTNAME}"
        rm -rf /var/lib/ceph/mds/mds-${MY_HOSTNAME}

    fi
    
    mkdir -p /var/lib/ceph/mds/mds-${MY_HOSTNAME}
    
    # install ceph mds key
    ceph auth get-or-create mds.${MY_HOSTNAME} mds 'allow ' osd 'allow *' mon 'allow rwx' > /var/lib/ceph/mds/mds-${MY_HOSTNAME}/keyring

    # done
    touch /var/lib/ceph/mds/mds-${MY_HOSTNAME}/done
    
    # Create & start & check Service
    Ceph_MDS_Create_Service
    Ceph_MDS_Start_Service
    Ceph_MDS_Status_Service
    Check_Return $0 $LINENO $FUNCNAME
    
    Log SUCC "ceph MDS install success"
}



######################################################################
# 作用: re-generate systemd for MDS service, use base ceph-mds@service
# 用法: Ceph_MDS_Create_Service
# 注意：
######################################################################
Ceph_MDS_Create_Service() {
    
    Log DEBUG "creating MDS service file"
    system_service_dir="/usr/lib/systemd/system"
    
    # src is default ceph
    src_mds_target="ceph-mds.target"
    src_mds_service_instance="ceph-mds@.service"

    # dst is ${CEPH_CLUSTERNAME}
    dst_mds_target="${CEPH_CLUSTERNAME}-mds.target"
    dst_mds_service_instance="${CEPH_CLUSTERNAME}-mds@.service"
    
    # copy from src to dst(if CEPH_CLUSTERNAME is not ceph)
    if [ "$src_mds_target" != "$dst_mds_target" ]; then
        cp -rLfap ${system_service_dir}/${src_mds_target} ${system_service_dir}/${dst_mds_target}
    fi
    if [ "$src_mds_service_instance" != "$dst_mds_service_instance" ]; then
        cp -rLfap ${system_service_dir}/${src_mds_service_instance} ${system_service_dir}/${dst_mds_service_instance}
    fi
    
    src_partof="ceph-mds.target"
    dst_partof="${CEPH_CLUSTERNAME}-mds.target"
    src_wantby="ceph-mds.target"
    dst_wantby="${CEPH_CLUSTERNAME}-mds.target"

    # replace the "Environment=CLUSTER=ceph"
    sed -i "s/Environment=CLUSTER=ceph/Environment=CLUSTER=${CEPH_CLUSTERNAME}/" \
    ${system_service_dir}/${dst_mds_service_instance}
    
    # replace the "--setuser ceph --setgroup ceph"
    sed -i "s/--setuser \(.*\) --setgroup \(.*\)/--setuser $CEPH_USER --setgroup ${CEPH_GROUP}/" \
    ${system_service_dir}/${dst_mds_service_instance}
    
    # replate the PartOf & WantedBy
    sed -i "s/PartOf=$src_partof/PartOf=$dst_partof/" ${system_service_dir}/${dst_mds_service_instance}
    sed -i "s/WantedBy=$src_wantby/WantedBy=$dst_wantby/" ${system_service_dir}/${dst_mds_service_instance}

    # auto start the service
    Run systemctl enable ceph.target
    Run systemctl enable ${CEPH_CLUSTERNAME}-mds.target
    
    Check_Return  $0 $LINENO $FUNCNAME
    Log SUCC "create MDS service file successful"
    
    
}


######################################################################
# 作用: remove systemd service for MDS service
# 用法: Ceph_MDS_Remove_Service
# 注意：
######################################################################
Ceph_MDS_Remove_Service() {

    # disable the service
    Log DEBUG "remove MDS service instance for cluster $CEPH_CLUSTERNAME"
    
    for i in `systemctl --plain | grep ${CEPH_CLUSTERNAME}-mds@[0-9] | awk '{print $1}'`; do
        Run systemctl disable $i
    done
    
    # flush the service instance
    Run systemctl reset-failed
    
    # disable the mds
    Run systemctl disable ${CEPH_CLUSTERNAME}-mds.target
    
    Log DEBUG "remove MDS service file for cluster $CEPH_CLUSTERNAME"
    system_service_dir="/usr/lib/systemd/system"

    # dst is ${CEPH_CLUSTERNAME}
    dst_mds_target="${CEPH_CLUSTERNAME}-mds.target"
    dst_mds_service_instance="${CEPH_CLUSTERNAME}-mds@.service"

    # if clustername is ceph, do not delete the service file
    if [ "$CEPH_CLUSTERNAME" != "ceph" ]; then
        if [ -f "${system_service_dir}/${dst_mds_target}" ]; then
            Run rm -rf ${system_service_dir}/${dst_mds_target}
        else
            Log DEBUG "${system_service_dir}/${dst_mds_target} does not exist"
        fi
        
        if [ -f "${system_service_dir}/${dst_mds_service_instance}" ]; then
            Run rm -rf ${system_service_dir}/${dst_mds_service_instance}
        else
            Log DEBUG "${system_service_dir}/${dst_mds_service_instance} does not exist"
        fi
    else
        Log DEBUG "clustername is $CEPH_CLUSTERNAME, no need to delete"
    fi
    
    Log SUCC "remove MDS service success"
    
    
}




######################################################################
# 作用: start systemd MDS service
# 用法: Ceph_MDS_Start_Service
# 注意：
######################################################################
Ceph_MDS_Start_Service() {
    Run systemctl start ${CEPH_CLUSTERNAME}-mds.target
    sleep 5
}




######################################################################
# 作用: stop systemd MDS service
# 用法: Ceph_MDS_Start_Service
# 注意：
######################################################################
Ceph_MDS_Stop_Service() {

    if systemctl stop ${CEPH_CLUSTERNAME}-mds.target; then
        sleep 5
        Log SUCC "stop MDS service successful"
    else
        Log WARN "systemctl stop ${CEPH_CLUSTERNAME}-mds.target failed, force kill"
        pid_to_kill=`ps -ef | grep "ceph-mds -f --cluster ${CEPH_CLUSTERNAME}" | grep -v grep | awk '{print $2}'`
        if [ -n "$pid_to_kill" ]; then
            kill -9 $pid_to_kill
            Log DEBUG "kill -9 $pid_to_kill"
        else
            Log WARN "not pid to kill, mybe service not running?"
        fi
    fi
    
    
    
}




######################################################################
# 作用: test MDS service status
# 用法: Ceph_MDS_Status_Service
# 注意：all running return0, some running return 10, no config return 1
######################################################################
Ceph_MDS_Status_Service() {
    
    Log DEBUG "checking MDS service status"
    
    # use ceph mds stat
    mds_stat=`$CEPH osd stat --connect-timeout 5`
    total_osd=`echo "$osd_stat" | grep osdmap | awk '{print $3}'`
    up_osd=`echo "$osd_stat" | grep osdmap | awk '{print $5}'`
    
    [ -z "$total_osd" ] && Log WARN "ceph OSD daemon not config" && return 1
    [ -z "$up_osd" ] && Log WARN "ceph OSD not running" && return 1    
    
    Log DEBUG "ceph MDS service with clustername $CEPH_CLUSTERNAME is running..."
}


######################################################################
# 作用: Install CEPH filesytem
# 用法: Ceph_FS_Install
# 注意：
######################################################################
Ceph_FS_Install() {

    Log DEBUG "install ceph filesystem"
    
    
    # fs depend on ceph mds
    Ceph_MDS_Install
    

    pg_num=`INI_Parser ${CEPH_CONF}  global "osd pool default pg num"`

    # get fs pool name CEPH_FS_POOL in settings.conf
    ceph_fs_pool_data=`echo $CEPH_FS_POOL | awk -F':' '{print $1}'`
    ceph_fs_pool_meta=`echo $CEPH_FS_POOL | awk -F':' '{print $2}'`
    [ -z "$ceph_fs_pool_data" -o -z "$ceph_fs_pool_meta" ] && Log ERROR "CEPH_FS_POOL in setting wrong"
    
    # create ceph fs
    $CEPH osd pool create $ceph_fs_pool_data $pg_num
    $CEPH osd pool create $cephfs_metadata $pg_num
    $CEPH fs new $CEPH_FS_NAME $ceph_fs_pool_meta $ceph_fs_pool_data

    # mod probe
    lsmod | grep -q ceph || Run modprobe ceph
    lsmod | grep -q ceph || Log ERROR "can NOT import ceph lib"

    
    # mount 
    Run mount -t ceph 192.168.0.184:6789:/ /home/cephfs/ -o name=admin,secret=`ceph-authtool -p /etc/ceph/ceph.client.admin.keyring`

    
    
    # add fstab
    #10.10.10.10:6789:/     /mnt/ceph    ceph    name=admin,secretfile=/etc/ceph/secret.key,noatime    0       2
    
    
    
    Log SUCC "ceph filesystem install success"
}






######################################################################
# 作用: Install CEPH RBD
# 用法: Ceph_RBD_Install
# 注意：
######################################################################
Ceph_RBD_Install() {
    Log DEBUG "install ceph block storage"

    :


    
    
    Log SUCC "ceph block storage install success"

}






######################################################################
# 作用: Install CEPH Object
# 用法: Ceph_Object_Install
# 注意：
######################################################################
Ceph_Object_Install() {
    Log DEBUG "install ceph object storage"

    :


    Log SUCC "ceph object storage install success"

}


######################################################################
# 作用: clear the Ceph_AI frame
# 用法: Ceph_Uninstall <clutername>
# 注意：
######################################################################
Ceph_AI_Clear() {

    :

}


######################################################################
# 作用: Uninstall the ceph cluter with given name
# 用法: Ceph_Uninstall <clutername>
# 注意：
######################################################################
Ceph_Uninstall() {
    :
    #Ceph_Mon_Uninstall
    #Ceph_OSD_Uninstall
    #Ceph_MDS_Uninstall
}


